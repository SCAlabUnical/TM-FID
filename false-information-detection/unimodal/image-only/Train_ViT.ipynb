{"cells":[{"cell_type":"code","execution_count":null,"id":"1f066aad-d5b0-45da-953f-4bd12289e08d","metadata":{"id":"1f066aad-d5b0-45da-953f-4bd12289e08d","outputId":"25c95651-bf65-4649-a973-73445ff089ff","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"]}],"source":["image_folder = 'untitled folder/gossipcop_images'\n","!pip install Pillow\n","!pip install torch\n","!pip install torchvision\n","\n","from PIL import Image\n","import os\n","import json\n","from torch.utils.data import Dataset, DataLoader  # Import DataLoader\n","import torchvision.transforms as transforms"]},{"cell_type":"code","execution_count":null,"id":"8670671d-ee4a-4ca6-b82b-eff1840f3321","metadata":{"id":"8670671d-ee4a-4ca6-b82b-eff1840f3321"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, image_folder, transform=None):\n","        self.root_dir = root_dir\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.data = self.load_data()\n","\n","    def load_data(self):\n","        data = []\n","\n","        for folder_name in os.listdir(self.root_dir)[:12000]:\n","            folder_path = os.path.join(self.root_dir, folder_name)\n","            json_path = os.path.join(folder_path, 'tweets.json')\n","\n","            if os.path.exists(json_path):\n","                try:\n","                    with open(json_path, 'r') as f:\n","                        tweets_data_list = json.load(f)\n","\n","                    tweet_number = folder_name.split('-')[-1]\n","                    image_name = f'{tweet_number}.jpg'\n","                    image_path = os.path.join(self.image_folder, image_name)\n","\n","                    if os.path.exists(image_path) and self.is_valid_image(image_path):\n","                        data.append({'text': tweets_data_list, 'image_path': image_path})\n","\n","                except json.JSONDecodeError as json_error:\n","                    print(f\"Error decoding JSON in file {json_path}: {json_error}\")\n","            else:\n","                print(f\"JSON file not found: {json_path}\")\n","\n","        return data\n","\n","    def is_valid_image(self, file_path):\n","        try:\n","            img = Image.open(file_path)\n","            img.verify()\n","            return True\n","        except Exception as e:\n","            print(f\"Invalid image file: {file_path}. Error: {e}\")\n","            return False\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        image_path = self.data[idx]['image_path']\n","\n","        try:\n","            image = Image.open(image_path).convert('RGB')\n","\n","            if self.transform:\n","                image = self.transform(image)\n","\n","            return {'text': text, 'image': image}\n","\n","        except OSError as e:\n","            print(f\"Error opening image file {image_path}: {e}\")\n","            return None"]},{"cell_type":"code","execution_count":null,"id":"96ce349b-ea58-4d06-a88a-96014885dbd6","metadata":{"id":"96ce349b-ea58-4d06-a88a-96014885dbd6"},"outputs":[],"source":["\n","root_folder = 'gossipcop_real'\n","image_folder = 'gossipcop_images'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","dataset = CustomDataset(root_folder, image_folder, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"9119d468-9486-4eb0-ba46-f2d5ba03d8c8","metadata":{"id":"9119d468-9486-4eb0-ba46-f2d5ba03d8c8"},"outputs":[],"source":["\n","root_folder = 'gossipcop_fake'\n","image_folder = 'gossipcop_images'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","dataset2 = CustomDataset(root_folder, image_folder, transform=transform)\n","dataloader2 = DataLoader(dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"7aff10ea-d240-41e2-8ef3-e48c60137fb3","metadata":{"id":"7aff10ea-d240-41e2-8ef3-e48c60137fb3"},"outputs":[],"source":["import numpy as np\n","\n","texts = []\n","images = []\n","\n","for batch in dataset:\n","    text = batch['text']\n","    image = batch['image']\n","\n","    if 'tweets' in text and text['tweets'] and len(text['tweets']) > 0:\n","        texts.append(text['tweets'][0]['text'])\n","    else:\n","        texts.append(\"\")\n","\n","    images.append(image)\n","\n","\n","texts_true = np.array(texts)\n","images_true = np.array(images)"]},{"cell_type":"code","execution_count":null,"id":"a67ad94a-6882-4b30-b9de-6544d5d9b82b","metadata":{"id":"a67ad94a-6882-4b30-b9de-6544d5d9b82b"},"outputs":[],"source":["texts2 = []\n","images2 = []\n","for batch2 in dataset2:\n","    text2 = batch2['text']\n","    image2 = batch2['image']\n","\n","\n","    if 'tweets' in text2 and text2['tweets'] and len(text2['tweets']) > 0:\n","        texts2.append(text2['tweets'][0]['text'])\n","    else:\n","        texts2.append(\"\")\n","\n","    images2.append(image2)\n","\n","texts_fake = np.array(texts2)\n","images_fake = np.array(images2)"]},{"cell_type":"code","execution_count":null,"id":"0d082c78-757c-45cd-a761-38a9be5882b9","metadata":{"id":"0d082c78-757c-45cd-a761-38a9be5882b9"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from transformers import AutoTokenizer, TFRobertaModel\n","\n","tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n","bert = TFRobertaModel.from_pretrained('vinai/bertweet-base')"]},{"cell_type":"code","execution_count":null,"id":"bf16b877-bd40-4c51-bc28-ae721d01b3e4","metadata":{"id":"bf16b877-bd40-4c51-bc28-ae721d01b3e4"},"outputs":[],"source":["max_len = 128\n","\n","texts_true_truncate = tokenizer(\n","    text=texts_true.tolist(),\n","    add_special_tokens=True,\n","    max_length=max_len,\n","    truncation=True,\n","    padding=True,\n","    return_tensors='tf',\n","    return_token_type_ids=False,\n","    return_attention_mask=True,\n","    verbose=True\n",")\n","\n","texts_fake_truncate = tokenizer(\n","    text=texts_fake.tolist(),\n","    add_special_tokens=True,\n","    max_length=max_len,\n","    truncation=True,\n","    padding=True,\n","    return_tensors='tf',\n","    return_token_type_ids=False,\n","    return_attention_mask=True,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"915c35fc-5bc0-44b1-ad59-d87b22dd5957","metadata":{"id":"915c35fc-5bc0-44b1-ad59-d87b22dd5957"},"outputs":[],"source":["from transformers import AutoImageProcessor, TFViTForImageClassification\n","\n","image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch32-224-in21k\")\n","model_img = TFViTForImageClassification.from_pretrained(\"google/vit-base-patch32-224-in21k\", output_hidden_states=True)"]},{"cell_type":"code","execution_count":null,"id":"137a6cc6-506e-4040-adee-f8f20661aef1","metadata":{"id":"137a6cc6-506e-4040-adee-f8f20661aef1"},"outputs":[],"source":["images_true = image_processor(\n","    images=images_true,\n","    return_tensors='tf',\n","    do_rescale=False\n",")['pixel_values']#.to(device)\n","images_true.shape\n","\n","print(\"htw\")\n","\n","images_fake = image_processor(\n","    images=images_fake,\n","    return_tensors='tf',\n","    do_rescale=False\n",")['pixel_values']\n","images_fake.shape"]},{"cell_type":"code","execution_count":null,"id":"739d40e1-2ad3-4a00-bbdb-aad6dc95ac8c","metadata":{"id":"739d40e1-2ad3-4a00-bbdb-aad6dc95ac8c"},"outputs":[],"source":["text_all_input_ids = []\n","text_all_attention_mask = []\n","text_all_input_ids = np.row_stack((texts_true_truncate['input_ids'],texts_fake_truncate['input_ids']))\n","text_all_attention_mask = np.row_stack((texts_true_truncate['attention_mask'],texts_fake_truncate['attention_mask']))\n","print(text_all_attention_mask.shape)"]},{"cell_type":"code","execution_count":null,"id":"e88fdf29-2b6b-4624-b821-c708e6364bd8","metadata":{"id":"e88fdf29-2b6b-4624-b821-c708e6364bd8"},"outputs":[],"source":["y_all = []\n","y_all.extend(np.zeros(len(texts_true)))\n","y_all.extend(np.ones(len(texts_fake)))\n","np.array(y_all)"]},{"cell_type":"code","execution_count":null,"id":"08b920d0-2c56-44c2-8ebd-d8d96227429b","metadata":{"id":"08b920d0-2c56-44c2-8ebd-d8d96227429b"},"outputs":[],"source":["img_all = []\n","img_all = np.row_stack((images_true,images_fake))\n","print(img_all.shape)"]},{"cell_type":"code","execution_count":null,"id":"d1b0cad6-2d46-442e-96eb-be54b0faa383","metadata":{"id":"d1b0cad6-2d46-442e-96eb-be54b0faa383"},"outputs":[],"source":["perm = np.random.permutation(len(text_all_input_ids))\n","text_all_input_ids = text_all_input_ids[perm]\n","text_all_attention_mask = text_all_attention_mask[perm]\n","img_all = img_all[perm]\n","y_all = np.array(y_all)[perm]"]},{"cell_type":"code","execution_count":null,"id":"1820191d-d27a-4ae3-b8f0-87b908d3b304","metadata":{"id":"1820191d-d27a-4ae3-b8f0-87b908d3b304"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_input_ids, test_input_ids, train_attention_masks, test_attention_mask, train_img, test_img, y_train, y_test = train_test_split(text_all_input_ids,text_all_attention_mask,img_all,y_all, test_size=0.20, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"86a4babc-2fd5-40e5-bf6d-05279cf5925f","metadata":{"id":"86a4babc-2fd5-40e5-bf6d-05279cf5925f"},"outputs":[],"source":["train_input_ids, val_input_ids, train_attention_masks, val_attention_masks, train_img , val_img,  y_train, y_val = train_test_split(train_input_ids,train_attention_masks, train_img,y_train, test_size=0.20, random_state=42)"]},{"cell_type":"markdown","source":["# IMG"],"metadata":{"id":"5NT5Tao8UpVi"},"id":"5NT5Tao8UpVi"},{"cell_type":"code","execution_count":null,"id":"967208eb-0b18-4be9-85f3-5d3dcf2fc24f","metadata":{"id":"967208eb-0b18-4be9-85f3-5d3dcf2fc24f"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Dense\n","\n","SEQ_LEN = 128\n","NUM_CLASSES= 1\n","COLOR_CHANNELS=images_fake.shape[1]\n","IMG_SIZE=images_fake.shape[2]\n","\n","# #VIT EMBEDDINGS\n","input_image = tf.keras.layers.Input((COLOR_CHANNELS, IMG_SIZE, IMG_SIZE), name=\"image\")\n","img_embedder = model_img.vit\n","input_img_embs = img_embedder(input_image)[0]\n","input_img_embs = input_img_embs[:,0,:]\n","\n","probs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(input_img_embs)\n","\n","\n","image_model = keras.Model(inputs=input_image, outputs=probs)\n","image_model.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3cf55eba-b325-4885-90f0-ebb2e0097c69","metadata":{"id":"3cf55eba-b325-4885-90f0-ebb2e0097c69"},"outputs":[],"source":["import tensorflow_addons as tfa\n","from keras.callbacks import ModelCheckpoint\n","\n","max_epochs = 8\n","batch_size = 32\n","opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n","loss = keras.losses.BinaryCrossentropy()\n","best_weights_file = \"fake_news_detection_weights_VITin.h5\"\n","auc = keras.metrics.AUC(curve=\"ROC\")\n","m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+auc.name, mode='max', verbose=2,\n","                          save_weights_only=True, save_best_only=True)\n","image_model.compile(loss=loss, optimizer=opt, metrics=[auc, keras.metrics.BinaryAccuracy()])\n"]},{"cell_type":"code","execution_count":null,"id":"daf2ef55-126a-4e8d-80dc-fb04bb4e89ff","metadata":{"id":"daf2ef55-126a-4e8d-80dc-fb04bb4e89ff"},"outputs":[],"source":["# test the model (classification report)\n","from sklearn.metrics import classification_report\n","best_weights_file = \"fake_news_detection_weights_VITin.h5\"\n","image_model.load_weights(best_weights_file)\n","opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n","image_model.compile(loss=loss, optimizer=opt, metrics=[auc, keras.metrics.BinaryAccuracy()])\n","\n","y_pred_probs = image_model.predict([test_img])\n","y_pred= [1 if x >=0.5 else 0 for x in y_pred_probs]\n","\n","report = classification_report(y_test, y_pred, digits=3)\n","print(report)"]},{"cell_type":"code","execution_count":null,"id":"32f97b4c-e01a-408b-8efe-4414b986f902","metadata":{"id":"32f97b4c-e01a-408b-8efe-4414b986f902"},"outputs":[],"source":["emb_model = keras.Model(inputs=input_image, outputs=image_model.layers[2].output)\n","img_all_embs = emb_model.predict(img_all)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}