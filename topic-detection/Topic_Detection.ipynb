{"cells":[{"cell_type":"markdown","source":["# Topic Detection"],"metadata":{"id":"_ixwS0tuPYaK"},"id":"_ixwS0tuPYaK"},{"cell_type":"markdown","source":["image folder represents the folder in which the images are placed"],"metadata":{"id":"iWskTO6OPfOH"},"id":"iWskTO6OPfOH"},{"cell_type":"code","execution_count":1,"id":"380d9468-823f-4275-a3bf-c8afc3049488","metadata":{"id":"380d9468-823f-4275-a3bf-c8afc3049488","executionInfo":{"status":"ok","timestamp":1709718706679,"user_tz":-60,"elapsed":10,"user":{"displayName":"Cristian Cosentino","userId":"00191828998197462057"}}},"outputs":[],"source":["image_folder = 'untitled folder/gossipcop_images'"]},{"cell_type":"code","execution_count":null,"id":"6a7ce66f-0af4-4c3b-a680-ea6dd2e985a2","metadata":{"scrolled":true,"id":"6a7ce66f-0af4-4c3b-a680-ea6dd2e985a2"},"outputs":[],"source":["!pip install Pillow\n","!pip install torch\n","!pip install torchvision\n","\n","from PIL import Image\n","import os\n","import json\n","from torch.utils.data import Dataset, DataLoader  # Import DataLoader\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","source":["The CustomDataset class includes several functions, including load_data, which allows you to read the different JSON files in the GossipCop dataset."],"metadata":{"id":"EIqjycFVPu3q"},"id":"EIqjycFVPu3q"},{"cell_type":"code","execution_count":null,"id":"f7d9d491-c4ca-411e-8d43-116c4e38341d","metadata":{"id":"f7d9d491-c4ca-411e-8d43-116c4e38341d"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, image_folder, transform=None):\n","        self.root_dir = root_dir\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.data = self.load_data()\n","\n","    def load_data(self):\n","        data = []\n","\n","        for folder_name in os.listdir(self.root_dir)[:5000]:\n","            folder_path = os.path.join(self.root_dir, folder_name)\n","            json_path = os.path.join(folder_path, 'tweets.json')\n","\n","            if os.path.exists(json_path):\n","                try:\n","                    with open(json_path, 'r') as f:\n","                        tweets_data_list = json.load(f)\n","\n","                    tweet_number = folder_name.split('-')[-1]\n","                    image_name = f'{tweet_number}.jpg'\n","                    image_path = os.path.join(self.image_folder, image_name)\n","\n","                    if os.path.exists(image_path) and self.is_valid_image(image_path):\n","                        data.append({'text': tweets_data_list, 'image_path': image_path})\n","\n","                except json.JSONDecodeError as json_error:\n","                    print(f\"Error decoding JSON in file {json_path}: {json_error}\")\n","            else:\n","                print(f\"JSON file not found: {json_path}\")\n","\n","        return data\n","\n","    def is_valid_image(self, file_path):\n","        try:\n","            # Tenta di aprire il file immagine con PIL\n","            img = Image.open(file_path)\n","            img.verify()\n","            return True\n","        except Exception as e:\n","            print(f\"Invalid image file: {file_path}. Error: {e}\")\n","            return False\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        image_path = self.data[idx]['image_path']\n","\n","        try:\n","            image = Image.open(image_path).convert('RGB')\n","\n","            if self.transform:\n","                image = self.transform(image)\n","\n","            return {'text': text, 'image': image}\n","\n","        except OSError as e:\n","            print(f\"Error opening image file {image_path}: {e}\")\n","            return None"]},{"cell_type":"markdown","source":["Real data is uploaded"],"metadata":{"id":"LxQ_YtGfQEDq"},"id":"LxQ_YtGfQEDq"},{"cell_type":"code","execution_count":null,"id":"aa0b1a0c-ec3d-4754-b6c6-3f840af4cd3f","metadata":{"id":"aa0b1a0c-ec3d-4754-b6c6-3f840af4cd3f"},"outputs":[],"source":["\n","root_folder = 'gossipcop_real'\n","image_folder = 'gossipcop_images'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","dataset = CustomDataset(root_folder, image_folder, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"]},{"cell_type":"markdown","source":["Fake data is uploaded"],"metadata":{"id":"1CR_XQsZQJ2L"},"id":"1CR_XQsZQJ2L"},{"cell_type":"code","execution_count":null,"id":"27ed1028-7059-44d7-98c1-5e1c4d610a7c","metadata":{"id":"27ed1028-7059-44d7-98c1-5e1c4d610a7c"},"outputs":[],"source":["# Esempio di utilizzo\n","root_folder = 'gossipcop_fake'\n","image_folder = 'gossipcop_images'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","dataset2 = CustomDataset(root_folder, image_folder, transform=transform)\n","dataloader2 = DataLoader(dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"4ef327c7-e082-42e7-b0f6-eb2083dcb14b","metadata":{"id":"4ef327c7-e082-42e7-b0f6-eb2083dcb14b"},"outputs":[],"source":["import numpy as np\n","\n","texts = []\n","images = []\n","\n","for batch in dataset:\n","    text = batch['text']\n","    image = batch['image']\n","\n","\n","    if 'tweets' in text and text['tweets'] and len(text['tweets']) > 0:\n","        texts.append(text['tweets'][0]['text'])\n","    else:\n","        texts.append(\"\")\n","\n","    images.append(image)\n","\n","\n","texts = np.array(texts)\n","images = np.array(images)"]},{"cell_type":"code","execution_count":null,"id":"9771df81-f1d0-4192-8b9c-b04c090cce9a","metadata":{"id":"9771df81-f1d0-4192-8b9c-b04c090cce9a"},"outputs":[],"source":["texts2 = []\n","images2 = []\n","for batch2 in dataset2:\n","    text2 = batch2['text']\n","    image2 = batch2['image']\n","\n","\n","    if 'tweets' in text2 and text2['tweets'] and len(text2['tweets']) > 0:\n","        texts2.append(text2['tweets'][0]['text'])\n","    else:\n","        texts2.append(\"\")\n","\n","    images2.append(image2)\n","\n","\n","texts2 = np.array(texts2)\n","images2 = np.array(images2)"]},{"cell_type":"code","execution_count":null,"id":"5d5c2e21-d1bf-4103-be37-c8707e5160e1","metadata":{"id":"5d5c2e21-d1bf-4103-be37-c8707e5160e1"},"outputs":[],"source":["ima = np.concatenate([images, images2], axis=0)\n","print(len(ima))"]},{"cell_type":"code","execution_count":null,"id":"04647fab-a4a7-4766-ad3e-561ebbcde994","metadata":{"id":"04647fab-a4a7-4766-ad3e-561ebbcde994"},"outputs":[],"source":["tex = np.concatenate([texts, texts2], axis=0)\n","\n","print(len(tex))"]},{"cell_type":"code","execution_count":null,"id":"dde27093-4c9f-4f0c-a617-a89e7e7bb00c","metadata":{"id":"dde27093-4c9f-4f0c-a617-a89e7e7bb00c"},"outputs":[],"source":["import numpy as np\n","from nltk.tokenize import word_tokenize\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def truncate_texts(texts, max_length=70):\n","    truncated_texts = []\n","\n","    for text in texts:\n","        tokens = word_tokenize(text)\n","        truncated_tokens = tokens[:max_length]\n","        truncated_text = \" \".join(truncated_tokens)\n","        truncated_texts.append(truncated_text)\n","\n","    return np.array(truncated_texts)\n","\n","\n","truncated_texts = truncate_texts(tex)\n","\n","print(truncated_texts)"]},{"cell_type":"code","execution_count":null,"id":"3d86f478-bf6f-4da5-839f-8c46bc6f55db","metadata":{"id":"3d86f478-bf6f-4da5-839f-8c46bc6f55db"},"outputs":[],"source":["tex=truncated_texts"]},{"cell_type":"code","execution_count":null,"id":"6788f5f9-f54e-4731-b133-9206a005c0ad","metadata":{"id":"6788f5f9-f54e-4731-b133-9206a005c0ad"},"outputs":[],"source":["from bertopic import BERTopic\n","from bertopic.representation import VisualRepresentation\n","\n","# Additional ways of representing a topic\n","visual_model = VisualRepresentation()\n","\n","# Make sure to add the `visual_model` to a dictionary\n","representation_model = {\n","   \"Visual_Aspect\":  visual_model,\n","}\n","topic_model = BERTopic(representation_model=representation_model, verbose=True)"]},{"cell_type":"markdown","source":["Embeddings of images and text in the same space are generated"],"metadata":{"id":"lNkYjxTBQcYV"},"id":"lNkYjxTBQcYV"},{"cell_type":"code","execution_count":null,"id":"956d400b-c3ea-4253-a032-2a200fdd2c61","metadata":{"id":"956d400b-c3ea-4253-a032-2a200fdd2c61"},"outputs":[],"source":["from bertopic.backend import MultiModalBackend\n","model = MultiModalBackend('clip-ViT-B-32', batch_size=32)\n","\n","# Embed both images and documents, then average them\n","doc_image_embeddings = model.embed(tex, ima)"]},{"cell_type":"code","execution_count":null,"id":"8f08ff08-7267-4a85-8b56-cbcebf85c56f","metadata":{"id":"8f08ff08-7267-4a85-8b56-cbcebf85c56f"},"outputs":[],"source":["from bertopic import BERTopic\n","from sklearn.cluster import KMeans\n","from sklearn.feature_extraction.text import CountVectorizer\n","from bertopic.representation import MaximalMarginalRelevance\n","vectorizer_model = CountVectorizer(stop_words=\"english\")\n","repr_model = MaximalMarginalRelevance(diversity = 0.3,  top_n_words = 15)\n","topic_model = BERTopic(vectorizer_model=vectorizer_model, representation_model=repr_model, verbose=True, min_topic_size = 50,  top_n_words = 15)\n","topics, probs = topic_model.fit_transform(tex, doc_image_embeddings)\n","captions[\"Topic\"] = topics"]},{"cell_type":"code","execution_count":null,"id":"b2e6c519-c377-4413-867d-820d3fef5795","metadata":{"id":"b2e6c519-c377-4413-867d-820d3fef5795"},"outputs":[],"source":["df_docs[\"Name\"].unique()"]},{"cell_type":"code","execution_count":null,"id":"94df5a2c-9a35-46dc-9228-ba90956009f9","metadata":{"id":"94df5a2c-9a35-46dc-9228-ba90956009f9"},"outputs":[],"source":["topic_model.save(name, save_embedding_model=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11 (ipykernel)","language":"python","name":"python3.11"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}